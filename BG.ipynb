{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Predictions From Binary Model Estimations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peter\\Anaconda3\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from patsy import dmatrices\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "# remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary Logistic Regression (BNL) can be used to estimate models with a binary target. Multinomial Logistic Regression (MNL) can be used to estimate models with discrete targets having 2 or more unordered pairs. While the BNL method is a special case of MNL, it is possible to specify and estimate a MNL model using separate BNL models for $J-1$ categories where $J$ is the reference category.\n",
    "\n",
    "This method is usually credited to Begg & Gray (1984) and its use is explained in detail in popular statistics texts (Allison, 2012; Hosemer & Lemeshow, 2004; Agresti, 2011).\n",
    "\n",
    "In the credit risk literature, it has been used to estimate delinquency states in transition matrices for mortgage portfolios (Grimshaw, et al. 2014; Constantinou, et al. 2010), model bank M&A outcomes (Koetter, et al. 2007), and is presented as a general method for estimating default and prepayment options (Castelli, 2012).\n",
    "\n",
    "This may not be an ideal option as there is a loss of efficiency, however, in certain circumstances it is preferable for computational reasons or to avoid convergence issues.\n",
    "\n",
    "### Estimation\n",
    "To use this approach, estimate $J-1$ binary logit models where for each model we reduce the training dataset to observations having the reference event type and the event type of interest only. In this way we are estimating models that compare\n",
    "$ P(Y=j|x) $ to $P(Y=J|x)$. See Agresti (2011) for more information on Baseline-Category Logits.\n",
    "\n",
    "### Model Correction\n",
    "While Begg & Gray (1984) showed that it is possible to estimate effects in this way, to make predictions, we need to correct the raw BNL model outputs. To understand this, note that the MNL model can be described in terms of the softmax function:\n",
    "\n",
    "$$ P(Y=j|X) = \\frac{e^{X\\beta_j}}{\\sum_{k=1}^{J} e^{X\\beta_k} }$$\n",
    "\n",
    "Note that with the Begg & Gray approach, we estimate models comparing the $j$th category to a baseline $J$ category. Since $e^{X\\beta_j} = \\frac{P(Y=j|X)}{P(Y=J|X)}$, is exactly what we estimate with the binary models, we simply take the sum of the separate $e^{X\\beta_j}$ estimated by binary models as the deminator to recover the MNL (softmax) probabilities.\n",
    "\n",
    "The example below describes this proces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_mnl(n, npreds, nlevels_of_y, rseed=212):\n",
    "    '''\n",
    "    simulates data based on a multinomial logistic regression (softmax) model\n",
    "    returns data (df) and the true coefficients (beta)\n",
    "    :param n: number of observations\n",
    "    :param npreds: number of predictors in x\n",
    "    :param nlevels_of_y: number of levels in y (j=1...J)\n",
    "    :param rseed: random seed\n",
    "    '''\n",
    "    random.seed(rseed)\n",
    "    np.random.seed(rseed)\n",
    "    # simulate standard normal covariates + regression coefficients\n",
    "    x = np.random.normal(size=[n, npreds])\n",
    "    beta = np.random.normal(size=[npreds, (nlevels_of_y-1)])\n",
    "    # n rows, nlevels_of_y-1 cols\n",
    "    e_xbeta = np.exp(x.dot(beta))\n",
    "    # $1 / (1 + \\sum_{j=1}^{nlevels_of_y-1} \\exp{X_j\\upbeta})$\n",
    "    inv_softmax_denom = 1 / (np.sum(e_xbeta, axis=1) + 1)\n",
    "    inv_softmax_denom = inv_softmax_denom[:, np.newaxis] # adds a new axis -> 2D array\n",
    "    # true probs for nlevels_of_y-1 models\n",
    "    p_true = np.multiply(e_xbeta, inv_softmax_denom)\n",
    "    # add complement so rows sum to 1\n",
    "    z = 1 - np.sum(p_true, axis=1)\n",
    "    z = z[:, np.newaxis]\n",
    "    p_true = np.append(p_true, z, axis=1)\n",
    "    # y - outcome\n",
    "    y = list()\n",
    "    for i in range(n):\n",
    "        outcome = np.random.multinomial(n=1,pvals=p_true[i,:]).argmax() + 1\n",
    "        y.append(outcome)\n",
    "    # return pandas dataframe of simulated data\n",
    "    df = pd.DataFrame(x)\n",
    "    df.columns = 'x' + df.columns.astype(str)  # rename cols x1, x2, ...\n",
    "    df['y'] = np.where(np.array(y)==3, 0, y)   # set last level (nlevels_of_y) to be 0\n",
    "    return(df, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate data\n",
    "df, beta = sim_mnl(n=1000, npreds=5, nlevels_of_y=3, rseed=212)\n",
    "\n",
    "# break into training and testing data\n",
    "# msk = np.random.rand(len(df)) < 0.5\n",
    "# df_train = df[msk]\n",
    "# df_test = df[~msk]\n",
    "\n",
    "# automatically create model formula\n",
    "mod_formula = 'y~' + '+'.join([i for i in df.columns if i!='y'])\n",
    "\n",
    "# remove to avoid accidentally using\n",
    "# del(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit MNL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.848411\n",
      "         Iterations 6\n"
     ]
    }
   ],
   "source": [
    "def fit_multinomial(data):\n",
    "    # generate endogenous and exogenous \n",
    "    Y, X = dmatrices(mod_formula, data=data, return_type='dataframe')\n",
    "    # fit model    \n",
    "    model = sm.MNLogit(Y, X)\n",
    "    results = model.fit()\n",
    "    return model, results\n",
    "\n",
    "mod_mnl, rslt_mnl = fit_multinomial(data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNL Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'scipy.stats' has no attribute 'chisqprob'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-4c8abdad199e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrslt_mnl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\statsmodels\\discrete\\discrete_model.py\u001b[0m in \u001b[0;36msummary\u001b[1;34m(self, yname, xname, title, alpha, yname_list)\u001b[0m\n\u001b[0;32m   2548\u001b[0m                      \u001b[1;33m(\u001b[0m\u001b[1;34m'Log-Likelihood:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2549\u001b[0m                      \u001b[1;33m(\u001b[0m\u001b[1;34m'LL-Null:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"%#8.5g\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mllnull\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2550\u001b[1;33m                      \u001b[1;33m(\u001b[0m\u001b[1;34m'LLR p-value:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"%#6.4g\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mllr_pvalue\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2551\u001b[0m                      ]\n\u001b[0;32m   2552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\statsmodels\\tools\\decorators.py\u001b[0m in \u001b[0;36m__get__\u001b[1;34m(self, obj, type)\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_cachedval\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[1;31m# Call the \"fget\" function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m             \u001b[0m_cachedval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m             \u001b[1;31m# Set the attribute in obj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m             \u001b[1;31m# print(\"Setting %s in cache to %s\" % (name, _cachedval))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\statsmodels\\discrete\\discrete_model.py\u001b[0m in \u001b[0;36mllr_pvalue\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2403\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mcache_readonly\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2404\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mllr_pvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2405\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchisqprob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mllr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2407\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mcache_readonly\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'scipy.stats' has no attribute 'chisqprob'"
     ]
    }
   ],
   "source": [
    "rslt_mnl.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNL Predicted Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y, X = dmatrices(mod_formula, data=df, return_type='dataframe')\n",
    "beta = rslt_mnl.params\n",
    "\n",
    "# calculate manually\n",
    "# verify equal to rslt_mnl.predict(X)\n",
    "e_xbeta = np.exp(np.dot(X,beta))\n",
    "inv_softmax_denom = 1 / (np.sum(e_xbeta, axis=1) + 1)\n",
    "inv_softmax_denom = inv_softmax_denom[:, np.newaxis] # adds a new axis -> 2D array\n",
    "pred_mnl = np.multiply(e_xbeta, inv_softmax_denom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the sum of predicted values adds up to the count of each category of Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(pred_mnl))\n",
    "df['y'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Separate BNL Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool1 = df['y'].isin([0,1])\n",
    "bool2 = df['y'].isin([0,2])\n",
    "\n",
    "def fit_binomial(data):\n",
    "    # generate endogenous and exogenous \n",
    "    Y, X = dmatrices(mod_formula, data=data, return_type='dataframe')\n",
    "    # set highest value of Y to be 1\n",
    "    Y = np.where(Y > 0, 1, 0)\n",
    "    # fit model\n",
    "    model = sm.Logit(Y, X)\n",
    "    results = model.fit()\n",
    "    return model, results\n",
    "    \n",
    "mod_bnl1, rslt_bnl1 = fit_binomial(data=df[bool1])\n",
    "mod_bnl2, rslt_bnl2 = fit_binomial(data=df[bool2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare MNL and BNL Coefficients\n",
    "Note coefficients for all inputs are very close, however, the intercepts are not necessarily close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_fits(rslt_mnl, rslt_bnl1, rslt_bnl2):\n",
    "    z = rslt_mnl.params\n",
    "    z.columns = ['mnl1', 'mnl2']\n",
    "    y = rslt_bnl1.params\n",
    "    y.name = 'bnl1'\n",
    "    z = z.join(y)\n",
    "    y = rslt_bnl2.params\n",
    "    y.name = 'bnl2' \n",
    "    z = z.join(y)\n",
    "    return(z)\n",
    "\n",
    "compare_fits(rslt_mnl, rslt_bnl1, rslt_bnl2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the software to predict MNL probs from BNL estimated models overestimates event probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta1 = rslt_bnl1.params\n",
    "beta2 = rslt_bnl2.params\n",
    "e_xbeta1 = np.exp(np.dot(X, beta1))\n",
    "e_xbeta2 = np.exp(np.dot(X, beta2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_bnl1_raw = e_xbeta1 / (1 + e_xbeta1)\n",
    "# Equivalent to: rslt_bnl1.predict(X).sum()\n",
    "p_bnl1_raw.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instead, need to use softmax to correct probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to add exp(logit) from all other (non reference) categories of Y. Note the sums of the predicted probabilities are close but not exactly equal to their multinomial equivalents and the true event counts. There is some loss of efficiency due to estimating models on smaller samples in the binary models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_bnl1_adj = e_xbeta1 / (1 + e_xbeta1 + e_xbeta2)\n",
    "p_bnl1_adj.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_bnl2_adj = e_xbeta2 / (1 + e_xbeta1 + e_xbeta2)\n",
    "p_bnl2_adj.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['y'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
