{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Predictions From Binary Model Estimations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from patsy import dmatrices\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "# remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary Logistic Regression (BNL) can be used to estimate models with a binary target. Multinomial Logistic Regression (MNL) can be used to estimate models with discrete targets having 2 or more unordered pairs. While the BNL method is a special case of MNL, it is possible to specify and estimate a MNL model using separate BNL models for $J-1$ categories where $J$ is the reference category.\n",
    "\n",
    "This method is usually credited to Begg & Gray (1984) and its use is explained in detail in popular statistics texts (Allison, 2012; Hosemer & Lemeshow, 2004; Agresti, 2011).\n",
    "\n",
    "In the credit risk literature, it has been used to estimate delinquency states in transition matrices for mortgage portfolios (Grimshaw, et al. 2014; Constantinou, et al. 2010), model bank M&A outcomes (Koetter, et al. 2007), and is presented as a general method for estimating default and prepayment options (Castelli, 2012).\n",
    "\n",
    "This may not be an ideal option as there is a loss of efficiency, however, in certain circumstances it is preferable for computational reasons or to avoid convergence issues.\n",
    "\n",
    "### Estimation\n",
    "To use this approach, estimate $J-1$ binary logit models where for each model we reduce the training dataset to observations having the reference event type and the event type of interest only. In this way we are estimating models that compare\n",
    "$ P(Y=j|x) $ to $P(Y=J|x)$. See Agresti (2011) for more information on Baseline-Category Logits.\n",
    "\n",
    "### Model Correction\n",
    "While Begg & Gray (1984) showed that it is possible to estimate effects in this way, to make predictions, we need to correct the raw BNL model outputs. To understand this, note that the MNL model can be described in terms of the softmax function:\n",
    "\n",
    "$$ P(Y=j|X) = \\frac{e^{X\\beta_j}}{\\sum_{k=1}^{J} e^{X\\beta_k} }$$\n",
    "\n",
    "Note that with the Begg & Gray approach, we estimate models comparing the $j$th category to a baseline $J$ category. Since $e^{X\\beta_j} = \\frac{P(Y=j|X)}{P(Y=J|X)}$, is exactly what we estimate with the binary models, we simply take the sum of the separate $e^{X\\beta_j}$ estimated by binary models as the deminator to recover the MNL (softmax) probabilities.\n",
    "\n",
    "The example below describes this proces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_mnl(n, npreds, nlevels_of_y, rseed=212):\n",
    "    '''\n",
    "    simulates data based on a multinomial logistic regression (softmax) model\n",
    "    returns data (df) and the true coefficients (beta)\n",
    "    :param n: number of observations\n",
    "    :param npreds: number of predictors in x\n",
    "    :param nlevels_of_y: number of levels in y (j=1...J)\n",
    "    :param rseed: random seed\n",
    "    '''\n",
    "    random.seed(rseed)\n",
    "    np.random.seed(rseed)\n",
    "    # simulate standard normal covariates + regression coefficients\n",
    "    x = np.random.normal(size=[n, npreds])\n",
    "    beta = np.random.normal(size=[npreds, (nlevels_of_y-1)])\n",
    "    # n rows, nlevels_of_y-1 cols\n",
    "    e_xbeta = np.exp(x.dot(beta))\n",
    "    # $1 / (1 + \\sum_{j=1}^{nlevels_of_y-1} \\exp{X_j\\upbeta})$\n",
    "    inv_softmax_denom = 1 / (np.sum(e_xbeta, axis=1) + 1)\n",
    "    inv_softmax_denom = inv_softmax_denom[:, np.newaxis] # adds a new axis -> 2D array\n",
    "    # true probs for nlevels_of_y-1 models\n",
    "    p_true = np.multiply(e_xbeta, inv_softmax_denom)\n",
    "    # add complement so rows sum to 1\n",
    "    z = 1 - np.sum(p_true, axis=1)\n",
    "    z = z[:, np.newaxis]\n",
    "    p_true = np.append(p_true, z, axis=1)\n",
    "    # y - outcome\n",
    "    y = list()\n",
    "    for i in range(n):\n",
    "        outcome = np.random.multinomial(n=1,pvals=p_true[i,:]).argmax() + 1\n",
    "        y.append(outcome)\n",
    "    # return pandas dataframe of simulated data\n",
    "    df = pd.DataFrame(x)\n",
    "    df.columns = 'x' + df.columns.astype(str)  # rename cols x1, x2, ...\n",
    "    df['y'] = np.where(np.array(y)==3, 0, y)   # set last level (nlevels_of_y) to be 0\n",
    "    return(df, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate data\n",
    "df, beta = sim_mnl(n=1000, npreds=5, nlevels_of_y=3, rseed=212)\n",
    "\n",
    "# break into training and testing data\n",
    "# msk = np.random.rand(len(df)) < 0.5\n",
    "# df_train = df[msk]\n",
    "# df_test = df[~msk]\n",
    "\n",
    "# automatically create model formula\n",
    "mod_formula = 'y~' + '+'.join([i for i in df.columns if i!='y'])\n",
    "\n",
    "# remove to avoid accidentally using\n",
    "# del(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit MNL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.848411\n",
      "         Iterations 6\n"
     ]
    }
   ],
   "source": [
    "def fit_multinomial(data):\n",
    "    # generate endogenous and exogenous \n",
    "    Y, X = dmatrices(mod_formula, data=data, return_type='dataframe')\n",
    "    # fit model    \n",
    "    model = sm.MNLogit(Y, X)\n",
    "    results = model.fit()\n",
    "    return model, results\n",
    "\n",
    "mod_mnl, rslt_mnl = fit_multinomial(data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNL Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>MNLogit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>y</td>        <th>  No. Observations:  </th>   <td>  1000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>              <td>MNLogit</td>     <th>  Df Residuals:      </th>   <td>   988</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>   <td>    10</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Sun, 19 Aug 2018</td> <th>  Pseudo R-squ.:     </th>   <td>0.2271</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>20:17:08</td>     <th>  Log-Likelihood:    </th>  <td> -848.41</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th>  <td> -1097.7</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th> <td>8.669e-101</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <th>y=1</th>       <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    0.0003</td> <td>    0.099</td> <td>    0.003</td> <td> 0.997</td> <td>   -0.193</td> <td>    0.194</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x0</th>        <td>    0.1642</td> <td>    0.100</td> <td>    1.648</td> <td> 0.099</td> <td>   -0.031</td> <td>    0.360</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>        <td>   -0.2948</td> <td>    0.086</td> <td>   -3.436</td> <td> 0.001</td> <td>   -0.463</td> <td>   -0.127</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>        <td>    0.3244</td> <td>    0.083</td> <td>    3.907</td> <td> 0.000</td> <td>    0.162</td> <td>    0.487</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>        <td>    0.5542</td> <td>    0.094</td> <td>    5.926</td> <td> 0.000</td> <td>    0.371</td> <td>    0.737</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>        <td>   -0.9926</td> <td>    0.100</td> <td>   -9.883</td> <td> 0.000</td> <td>   -1.189</td> <td>   -0.796</td>\n",
       "</tr>\n",
       "<tr>\n",
       "     <th>y=2</th>       <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   -0.0247</td> <td>    0.100</td> <td>   -0.246</td> <td> 0.806</td> <td>   -0.222</td> <td>    0.172</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x0</th>        <td>   -1.1488</td> <td>    0.111</td> <td>  -10.322</td> <td> 0.000</td> <td>   -1.367</td> <td>   -0.931</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>        <td>   -0.7355</td> <td>    0.093</td> <td>   -7.912</td> <td> 0.000</td> <td>   -0.918</td> <td>   -0.553</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>        <td>    0.3437</td> <td>    0.087</td> <td>    3.963</td> <td> 0.000</td> <td>    0.174</td> <td>    0.514</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>        <td>   -0.5313</td> <td>    0.096</td> <td>   -5.512</td> <td> 0.000</td> <td>   -0.720</td> <td>   -0.342</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>        <td>   -0.4641</td> <td>    0.098</td> <td>   -4.728</td> <td> 0.000</td> <td>   -0.656</td> <td>   -0.272</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                          MNLogit Regression Results                          \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   No. Observations:                 1000\n",
       "Model:                        MNLogit   Df Residuals:                      988\n",
       "Method:                           MLE   Df Model:                           10\n",
       "Date:                Sun, 19 Aug 2018   Pseudo R-squ.:                  0.2271\n",
       "Time:                        20:17:08   Log-Likelihood:                -848.41\n",
       "converged:                       True   LL-Null:                       -1097.7\n",
       "                                        LLR p-value:                8.669e-101\n",
       "==============================================================================\n",
       "       y=1       coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept      0.0003      0.099      0.003      0.997      -0.193       0.194\n",
       "x0             0.1642      0.100      1.648      0.099      -0.031       0.360\n",
       "x1            -0.2948      0.086     -3.436      0.001      -0.463      -0.127\n",
       "x2             0.3244      0.083      3.907      0.000       0.162       0.487\n",
       "x3             0.5542      0.094      5.926      0.000       0.371       0.737\n",
       "x4            -0.9926      0.100     -9.883      0.000      -1.189      -0.796\n",
       "------------------------------------------------------------------------------\n",
       "       y=2       coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     -0.0247      0.100     -0.246      0.806      -0.222       0.172\n",
       "x0            -1.1488      0.111    -10.322      0.000      -1.367      -0.931\n",
       "x1            -0.7355      0.093     -7.912      0.000      -0.918      -0.553\n",
       "x2             0.3437      0.087      3.963      0.000       0.174       0.514\n",
       "x3            -0.5313      0.096     -5.512      0.000      -0.720      -0.342\n",
       "x4            -0.4641      0.098     -4.728      0.000      -0.656      -0.272\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rslt_mnl.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNL Predicted Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y, X = dmatrices(mod_formula, data=df, return_type='dataframe')\n",
    "beta = rslt_mnl.params\n",
    "\n",
    "# calculate manually\n",
    "# verify equal to rslt_mnl.predict(X)\n",
    "e_xbeta = np.exp(np.dot(X,beta))\n",
    "inv_softmax_denom = 1 / (np.sum(e_xbeta, axis=1) + 1)\n",
    "inv_softmax_denom = inv_softmax_denom[:, np.newaxis] # adds a new axis -> 2D array\n",
    "pred_mnl = np.multiply(e_xbeta, inv_softmax_denom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the sum of predicted values adds up to the count of each category of Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[336. 349.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2    349\n",
       "1    336\n",
       "0    315\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sum(pred_mnl))\n",
    "df['y'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Separate BNL Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.558735\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.513672\n",
      "         Iterations 6\n"
     ]
    }
   ],
   "source": [
    "bool1 = df['y'].isin([0,1])\n",
    "bool2 = df['y'].isin([0,2])\n",
    "\n",
    "def fit_binomial(data):\n",
    "    # generate endogenous and exogenous \n",
    "    Y, X = dmatrices(mod_formula, data=data, return_type='dataframe')\n",
    "    # set highest value of Y to be 1\n",
    "    Y = np.where(Y > 0, 1, 0)\n",
    "    # fit model\n",
    "    model = sm.Logit(Y, X)\n",
    "    results = model.fit()\n",
    "    return model, results\n",
    "    \n",
    "mod_bnl1, rslt_bnl1 = fit_binomial(data=df[bool1])\n",
    "mod_bnl2, rslt_bnl2 = fit_binomial(data=df[bool2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare MNL and BNL Coefficients\n",
    "Note coefficients for all inputs are very close, however, the intercepts are not necessarily close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mnl1</th>\n",
       "      <th>mnl2</th>\n",
       "      <th>bnl1</th>\n",
       "      <th>bnl2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Intercept</th>\n",
       "      <td>0.000331</td>\n",
       "      <td>-0.024687</td>\n",
       "      <td>-0.002811</td>\n",
       "      <td>-0.002215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x0</th>\n",
       "      <td>0.164217</td>\n",
       "      <td>-1.148827</td>\n",
       "      <td>0.188774</td>\n",
       "      <td>-1.130944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x1</th>\n",
       "      <td>-0.294826</td>\n",
       "      <td>-0.735451</td>\n",
       "      <td>-0.301857</td>\n",
       "      <td>-0.740613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x2</th>\n",
       "      <td>0.324396</td>\n",
       "      <td>0.343721</td>\n",
       "      <td>0.342078</td>\n",
       "      <td>0.363991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x3</th>\n",
       "      <td>0.554198</td>\n",
       "      <td>-0.531287</td>\n",
       "      <td>0.548890</td>\n",
       "      <td>-0.487318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x4</th>\n",
       "      <td>-0.992556</td>\n",
       "      <td>-0.464052</td>\n",
       "      <td>-0.981425</td>\n",
       "      <td>-0.455103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mnl1      mnl2      bnl1      bnl2\n",
       "Intercept  0.000331 -0.024687 -0.002811 -0.002215\n",
       "x0         0.164217 -1.148827  0.188774 -1.130944\n",
       "x1        -0.294826 -0.735451 -0.301857 -0.740613\n",
       "x2         0.324396  0.343721  0.342078  0.363991\n",
       "x3         0.554198 -0.531287  0.548890 -0.487318\n",
       "x4        -0.992556 -0.464052 -0.981425 -0.455103"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_fits(rslt_mnl, rslt_bnl1, rslt_bnl2):\n",
    "    z = rslt_mnl.params\n",
    "    z.columns = ['mnl1', 'mnl2']\n",
    "    y = rslt_bnl1.params\n",
    "    y.name = 'bnl1'\n",
    "    z = z.join(y)\n",
    "    y = rslt_bnl2.params\n",
    "    y.name = 'bnl2' \n",
    "    z = z.join(y)\n",
    "    return(z)\n",
    "\n",
    "compare_fits(rslt_mnl, rslt_bnl1, rslt_bnl2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the software to predict MNL probs from BNL estimated models overestimates event probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta1 = rslt_bnl1.params\n",
    "beta2 = rslt_bnl2.params\n",
    "e_xbeta1 = np.exp(np.dot(X, beta1))\n",
    "e_xbeta2 = np.exp(np.dot(X, beta2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "496.8677490119678"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_bnl1_raw = e_xbeta1 / (1 + e_xbeta1)\n",
    "# Equivalent to: rslt_bnl1.predict(X).sum()\n",
    "p_bnl1_raw.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instead, need to use softmax to correct probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to add exp(logit) from all other (non reference) categories of Y. Note the sums of the predicted probabilities are close but not exactly equal to their multinomial equivalents and the true event counts. There is some loss of efficiency due to estimating models on smaller samples in the binary models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "333.9213242796692"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_bnl1_adj = e_xbeta1 / (1 + e_xbeta1 + e_xbeta2)\n",
    "p_bnl1_adj.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "351.6671766171102"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_bnl2_adj = e_xbeta2 / (1 + e_xbeta1 + e_xbeta2)\n",
    "p_bnl2_adj.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    349\n",
       "1    336\n",
       "0    315\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['y'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
